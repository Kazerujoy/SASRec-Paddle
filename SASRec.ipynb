{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle \n",
    "from paddle import nn \n",
    "import numpy as np \n",
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class SASRec(nn.Layer):\n",
    "    def __init__(self, user_num, item_num, batch_size, lr, maxlen, hidden_units, num_blocks, num_epochs, num_heads, dropout_rate, l2_emb):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        \n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        \n",
    "        self.dev = paddle.get_device()\n",
    "\n",
    "        self.item_emb = nn.Embedding(self.item_num+1, hidden_units, padding_idx=1)\n",
    "        self.pos_emb = nn.Embedding(maxlen, hidden_units) \n",
    "        self.emb_dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        transencoderlayer = nn.TransformerEncoderLayer(d_model= hidden_units, nhead= num_heads, dropout= dropout_rate, dim_feedforward= hidden_units, normalize_before=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer = transencoderlayer, num_layers= num_blocks )\n",
    "\n",
    "\n",
    "    def log2feats(self, log_seqs):\n",
    "\n",
    "        #pos_emb\n",
    "        seqs = self.item_emb(paddle.to_tensor(log_seqs,dtype='int64'))\n",
    "        #seqs *= log_seqs.shape[0] ** 0.5\n",
    "        positions = np.tile(np.array(range(seqs.shape[1])), [seqs.shape[0], 1])\n",
    "        positions = self.pos_emb(paddle.to_tensor(positions))\n",
    "        seqs_embed = self.emb_dropout(seqs + positions)\n",
    "        \n",
    "        attention_mask = paddle.triu(paddle.ones((maxlen, maxlen)))==0\n",
    "        log_feats = self.encoder(seqs_embed,attention_mask)\n",
    "\n",
    "        # log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)\n",
    "\n",
    "        return log_feats\n",
    "\n",
    "    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs): # for training        \n",
    "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
    "\n",
    "        pos_embs = self.item_emb(paddle.to_tensor(pos_seqs))\n",
    "        neg_embs = self.item_emb(paddle.to_tensor(neg_seqs))\n",
    "\n",
    "        pos_logits = (log_feats * pos_embs).sum(axis=-1)\n",
    "        neg_logits = (log_feats * neg_embs).sum(axis=-1)\n",
    "\n",
    "        return pos_logits, neg_logits # pos_pred, neg_pred\n",
    "\n",
    "    def predict(self, user_ids, log_seqs, item_indices): # for inference\n",
    "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
    "\n",
    "        final_feat = log_feats[:, -1, :] # only use last QKV classifier, a waste\n",
    "\n",
    "        item_embs = self.item_emb(paddle.to_tensor(np.array(item_indices).astype(np.int64))) # (U, I, C)\n",
    "\n",
    "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return logits # preds # (U, I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = \"ml-1m\"\r\n",
    "batch_size = 128\r\n",
    "lr = 0.001\r\n",
    "maxlen = 200\r\n",
    "hidden_units = 50\r\n",
    "num_blocks = 2\r\n",
    "num_epochs = 600\r\n",
    "num_heads = 1\r\n",
    "dropout_rate = 0.2\r\n",
    "l2_emb = 0\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_train, user_valid, user_test, usernum, itemnum = data_partition(dataset)\r\n",
    "num_batch = len(user_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sequence length: 163.50\n"
     ]
    }
   ],
   "source": [
    "cc = 0.0\r\n",
    "for u in user_train:\r\n",
    "    cc += len(user_train[u])\r\n",
    "print('average sequence length: %.2f' % (cc / len(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(dataset + '_' + 'log.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)\r\n",
    "model = SASRec(usernum, itemnum, batch_size, lr, maxlen, hidden_units, num_blocks, num_epochs, num_heads, dropout_rate, l2_emb)\r\n",
    "\r\n",
    "bce_criterion = nn.BCEWithLogitsLoss() \r\n",
    "adam_optimizer = paddle.optimizer.Adam(parameters= model.parameters(), beta1= 0.9, beta2= 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 0 : 1.2351962327957153\n",
      "loss in epoch 1 : 1.1841678238929587\n",
      "loss in epoch 2 : 1.1688527908731015\n",
      "loss in epoch 3 : 1.1472041124993182\n",
      "loss in epoch 4 : 1.113295499314653\n",
      "loss in epoch 5 : 1.0984528901729178\n",
      "loss in epoch 6 : 1.0824620495451258\n",
      "loss in epoch 7 : 1.0680202220348602\n",
      "loss in epoch 8 : 1.054815723540935\n",
      "loss in epoch 9 : 1.0438662412318778\n",
      "loss in epoch 10 : 1.0376609234099692\n",
      "loss in epoch 11 : 1.0305345197941393\n",
      "loss in epoch 12 : 1.024889738001722\n",
      "loss in epoch 13 : 1.0161835728807653\n",
      "loss in epoch 14 : 1.0114926454868722\n",
      "loss in epoch 15 : 1.0071433769895675\n",
      "loss in epoch 16 : 1.0049965990350602\n",
      "loss in epoch 17 : 0.9983519543992713\n",
      "loss in epoch 18 : 0.9984757012509285\n",
      "loss in epoch 19 : 1.0021409011901694\n",
      "loss in epoch 20 : 0.9881873663435591\n",
      "loss in epoch 21 : 0.9828925335660894\n",
      "loss in epoch 22 : 0.9761358575618013\n",
      "loss in epoch 23 : 0.9751568474668137\n",
      "loss in epoch 24 : 0.9750050940412156\n",
      "loss in epoch 25 : 0.9703084141650098\n",
      "loss in epoch 26 : 0.9666707858126214\n",
      "loss in epoch 27 : 0.9657544569766268\n",
      "loss in epoch 28 : 0.9581267009390161\n",
      "loss in epoch 29 : 0.9573154056325872\n",
      "loss in epoch 30 : 0.9546322366024586\n",
      "loss in epoch 31 : 0.957502286484901\n",
      "loss in epoch 32 : 0.9510622557173384\n",
      "loss in epoch 33 : 0.9475766483773577\n",
      "loss in epoch 34 : 0.9575950338485393\n",
      "loss in epoch 35 : 0.9473883560363282\n",
      "loss in epoch 36 : 0.9499927847943408\n",
      "loss in epoch 37 : 0.9412705720739162\n",
      "loss in epoch 38 : 0.9447961043804249\n",
      "loss in epoch 39 : 0.9383275305971186\n",
      "loss in epoch 40 : 0.9388604975761251\n",
      "Evaluating............................................................epoch:40, time: 69.600192(s), valid (NDCG@10: 0.5398, HR@10: 0.7947)\n",
      "loss in epoch 41 : 0.936999241088299\n",
      "loss in epoch 42 : 0.9345679524097037\n",
      "loss in epoch 43 : 0.9339644959632386\n",
      "loss in epoch 44 : 0.9361971806972584\n",
      "loss in epoch 45 : 0.9321791681837528\n",
      "loss in epoch 46 : 0.9413160440769601\n",
      "loss in epoch 47 : 0.9360590323488763\n",
      "loss in epoch 48 : 0.9302242816762721\n",
      "loss in epoch 49 : 0.9337788604675455\n",
      "loss in epoch 50 : 0.936803972467463\n",
      "loss in epoch 51 : 0.9296085783775817\n",
      "loss in epoch 52 : 0.9327626837060806\n",
      "loss in epoch 53 : 0.934693916046873\n",
      "loss in epoch 54 : 0.9282482697608623\n",
      "loss in epoch 55 : 0.9280803761583694\n",
      "loss in epoch 56 : 0.9310252286018209\n",
      "loss in epoch 57 : 0.9257215844823959\n",
      "loss in epoch 58 : 0.9234705719541996\n",
      "loss in epoch 59 : 0.9295422789898324\n",
      "loss in epoch 60 : 0.9305875301361084\n",
      "loss in epoch 61 : 0.9261641464334853\n",
      "loss in epoch 62 : 0.9257262691538385\n",
      "loss in epoch 63 : 0.922626756607218\n",
      "loss in epoch 64 : 0.9312656775433966\n",
      "loss in epoch 65 : 0.921156070333846\n",
      "loss in epoch 66 : 0.916767049343028\n",
      "loss in epoch 67 : 0.9259425021232442\n",
      "loss in epoch 68 : 0.9254531911078919\n",
      "loss in epoch 69 : 0.9252627136859488\n",
      "loss in epoch 70 : 0.9229822932405675\n",
      "loss in epoch 71 : 0.919463131021946\n",
      "loss in epoch 72 : 0.925110989428581\n",
      "loss in epoch 73 : 0.9213309262661223\n",
      "loss in epoch 74 : 0.9295810080589132\n",
      "loss in epoch 75 : 0.9151984085427954\n",
      "loss in epoch 76 : 0.9183509476641392\n",
      "loss in epoch 77 : 0.9201321234094336\n",
      "loss in epoch 78 : 0.9173874994541736\n",
      "loss in epoch 79 : 0.9168413027803949\n",
      "loss in epoch 80 : 0.916222962927311\n",
      "Evaluating............................................................epoch:80, time: 137.675300(s), valid (NDCG@10: 0.5777, HR@10: 0.8174)\n",
      "loss in epoch 81 : 0.9182115438136649\n",
      "loss in epoch 82 : 0.9136962839897643\n",
      "loss in epoch 83 : 0.9180638878903491\n",
      "loss in epoch 84 : 0.913476197009391\n",
      "loss in epoch 85 : 0.9155420559517881\n",
      "loss in epoch 86 : 0.9117164637180085\n",
      "loss in epoch 87 : 0.9145785849145118\n",
      "loss in epoch 88 : 0.9214801420556739\n",
      "loss in epoch 89 : 0.9124473384086121\n",
      "loss in epoch 90 : 0.910551019171451\n",
      "loss in epoch 91 : 0.9217090061370362\n",
      "loss in epoch 92 : 0.9102813246402335\n",
      "loss in epoch 93 : 0.9142867161872539\n",
      "loss in epoch 94 : 0.9196022279719089\n",
      "loss in epoch 95 : 0.9186096406997518\n",
      "loss in epoch 96 : 0.912950667929142\n",
      "loss in epoch 97 : 0.9146331371145046\n",
      "loss in epoch 98 : 0.9165332685125634\n",
      "loss in epoch 99 : 0.9114524443098839\n",
      "loss in epoch 100 : 0.9040859240166684\n",
      "loss in epoch 101 : 0.9087197032380612\n",
      "loss in epoch 102 : 0.9102630399643107\n",
      "loss in epoch 103 : 0.9105563785167451\n",
      "loss in epoch 104 : 0.9110197462934129\n",
      "loss in epoch 105 : 0.9173795200408773\n",
      "loss in epoch 106 : 0.9079649879577312\n",
      "loss in epoch 107 : 0.912632924445132\n",
      "loss in epoch 108 : 0.9128702701406276\n",
      "loss in epoch 109 : 0.9095939562675801\n",
      "loss in epoch 110 : 0.9027576218260095\n",
      "loss in epoch 111 : 0.9152384007230718\n",
      "loss in epoch 112 : 0.909108300158318\n",
      "loss in epoch 113 : 0.9134602115509358\n",
      "loss in epoch 114 : 0.911418819681127\n",
      "loss in epoch 115 : 0.9081143318338597\n",
      "loss in epoch 116 : 0.9154636986712192\n",
      "loss in epoch 117 : 0.9078254420706566\n",
      "loss in epoch 118 : 0.9044852751366635\n",
      "loss in epoch 119 : 0.9021620369972067\n",
      "loss in epoch 120 : 0.9091987698636157\n",
      "Evaluating............................................................epoch:120, time: 205.871001(s), valid (NDCG@10: 0.5879, HR@10: 0.8228)\n",
      "loss in epoch 121 : 0.9107909595712702\n",
      "loss in epoch 122 : 0.9078609195161373\n",
      "loss in epoch 123 : 0.9098365243445051\n",
      "loss in epoch 124 : 0.906625488971142\n",
      "loss in epoch 125 : 0.9058614132252145\n",
      "loss in epoch 126 : 0.9001680688655123\n",
      "loss in epoch 127 : 0.8982249079866612\n",
      "loss in epoch 128 : 0.9097541900391274\n",
      "loss in epoch 129 : 0.9058560335889776\n",
      "loss in epoch 130 : 0.9126763077492409\n",
      "loss in epoch 131 : 0.9073101244074233\n",
      "loss in epoch 132 : 0.9095050908149557\n",
      "loss in epoch 133 : 0.901983731604637\n",
      "loss in epoch 134 : 0.9077853687266086\n",
      "loss in epoch 135 : 0.9042128869827758\n",
      "loss in epoch 136 : 0.901511123839845\n",
      "loss in epoch 137 : 0.8978120342214057\n",
      "loss in epoch 138 : 0.9060159376327027\n",
      "loss in epoch 139 : 0.9022254385846726\n",
      "loss in epoch 140 : 0.9052936257200038\n",
      "loss in epoch 141 : 0.9091410611538177\n",
      "loss in epoch 142 : 0.9055137317231361\n",
      "loss in epoch 143 : 0.9077558948638591\n",
      "loss in epoch 144 : 0.9051570486515126\n",
      "loss in epoch 145 : 0.8972191176515945\n",
      "loss in epoch 146 : 0.9112498760223389\n",
      "loss in epoch 147 : 0.9011044755895087\n",
      "loss in epoch 148 : 0.9020259633977362\n",
      "loss in epoch 149 : 0.897764559755934\n",
      "loss in epoch 150 : 0.8978147836441689\n",
      "loss in epoch 151 : 0.9043798395927917\n",
      "loss in epoch 152 : 0.9038653297627226\n",
      "loss in epoch 153 : 0.9016562738317124\n",
      "loss in epoch 154 : 0.9011248337461594\n",
      "loss in epoch 155 : 0.9028851427930467\n",
      "loss in epoch 156 : 0.8955011063433708\n",
      "loss in epoch 157 : 0.896143432627333\n",
      "loss in epoch 158 : 0.897184505107555\n",
      "loss in epoch 159 : 0.9071767482351749\n",
      "loss in epoch 160 : 0.9031266443272854\n",
      "Evaluating............................................................epoch:160, time: 273.795357(s), valid (NDCG@10: 0.5978, HR@10: 0.8293)\n",
      "loss in epoch 161 : 0.9056559927920078\n",
      "loss in epoch 162 : 0.9051576941571338\n",
      "loss in epoch 163 : 0.9048147214219925\n",
      "loss in epoch 164 : 0.8990509700267872\n",
      "loss in epoch 165 : 0.9024540502974328\n",
      "loss in epoch 166 : 0.8965788749938316\n",
      "loss in epoch 167 : 0.9011168961829328\n",
      "loss in epoch 168 : 0.8973073807168515\n",
      "loss in epoch 169 : 0.9036145400493703\n",
      "loss in epoch 170 : 0.9016241418554428\n",
      "loss in epoch 171 : 0.8989601756664033\n",
      "loss in epoch 172 : 0.8996862654990339\n",
      "loss in epoch 173 : 0.9118361485765335\n",
      "loss in epoch 174 : 0.8965541695026641\n",
      "loss in epoch 175 : 0.891051636097279\n",
      "loss in epoch 176 : 0.8866081504111595\n",
      "loss in epoch 177 : 0.8993010241934594\n",
      "loss in epoch 178 : 0.8944338851786674\n",
      "loss in epoch 179 : 0.8994296117031828\n",
      "loss in epoch 180 : 0.8994778584926686\n",
      "loss in epoch 181 : 0.9009487730391482\n",
      "loss in epoch 182 : 0.8970437506411938\n",
      "loss in epoch 183 : 0.8856573333131507\n",
      "loss in epoch 184 : 0.897432507352626\n",
      "loss in epoch 185 : 0.8981603333290588\n",
      "loss in epoch 186 : 0.8987994460349388\n",
      "loss in epoch 187 : 0.8910086991939139\n",
      "loss in epoch 188 : 0.8935176019972944\n",
      "loss in epoch 189 : 0.894204151123128\n",
      "loss in epoch 190 : 0.8975899726786511\n",
      "loss in epoch 191 : 0.9017270017177501\n",
      "loss in epoch 192 : 0.8996201918480244\n",
      "loss in epoch 193 : 0.8978505185309876\n",
      "loss in epoch 194 : 0.8904172296219683\n",
      "loss in epoch 195 : 0.8910772496081413\n",
      "loss in epoch 196 : 0.8938475758471387\n",
      "loss in epoch 197 : 0.8958088844380481\n",
      "loss in epoch 198 : 0.9018316078693309\n",
      "loss in epoch 199 : 0.8929865728033349\n",
      "loss in epoch 200 : 0.8924988812588631\n",
      "Evaluating............................................................epoch:200, time: 342.186632(s), valid (NDCG@10: 0.6047, HR@10: 0.8329)\n",
      "loss in epoch 201 : 0.889242670637496\n",
      "loss in epoch 202 : 0.8936720794819771\n",
      "loss in epoch 203 : 0.9009085094675104\n",
      "loss in epoch 204 : 0.8935166001319885\n",
      "loss in epoch 205 : 0.8993128943950572\n",
      "loss in epoch 206 : 0.8887403632732148\n",
      "loss in epoch 207 : 0.8921535357515863\n",
      "loss in epoch 208 : 0.8888161397994833\n",
      "loss in epoch 209 : 0.8910398965186261\n",
      "loss in epoch 210 : 0.8981672106905186\n",
      "loss in epoch 211 : 0.903250053842017\n",
      "loss in epoch 212 : 0.8930124716555818\n",
      "loss in epoch 213 : 0.9097841458117708\n",
      "loss in epoch 214 : 0.8860370529458877\n",
      "loss in epoch 215 : 0.8911633580288989\n",
      "loss in epoch 216 : 0.8943476892532186\n",
      "loss in epoch 217 : 0.8878707784287473\n",
      "loss in epoch 218 : 0.8973778093114813\n",
      "loss in epoch 219 : 0.8898752813643598\n",
      "loss in epoch 220 : 0.8969866088096131\n",
      "loss in epoch 221 : 0.884950947254262\n",
      "loss in epoch 222 : 0.8997518407537582\n",
      "loss in epoch 223 : 0.8939984922713422\n",
      "loss in epoch 224 : 0.892548605482629\n",
      "loss in epoch 225 : 0.8945525339309205\n",
      "loss in epoch 226 : 0.8977247108804419\n",
      "loss in epoch 227 : 0.8943311889120873\n",
      "loss in epoch 228 : 0.8888212746762215\n",
      "loss in epoch 229 : 0.9014351735723779\n",
      "loss in epoch 230 : 0.898918867111206\n",
      "loss in epoch 231 : 0.8887067589354007\n",
      "loss in epoch 232 : 0.8867991921749521\n",
      "loss in epoch 233 : 0.8910649469558228\n",
      "loss in epoch 234 : 0.891089436855722\n",
      "loss in epoch 235 : 0.8929806313616164\n",
      "loss in epoch 236 : 0.8902074326860144\n",
      "loss in epoch 237 : 0.8988064781148383\n",
      "loss in epoch 238 : 0.8959486560618624\n",
      "loss in epoch 239 : 0.897276019796412\n",
      "loss in epoch 240 : 0.8879131294311361\n",
      "Evaluating............................................................epoch:240, time: 410.100781(s), valid (NDCG@10: 0.6090, HR@10: 0.8387)\n",
      "loss in epoch 241 : 0.8854612670046218\n",
      "loss in epoch 242 : 0.8924440652766126\n",
      "loss in epoch 243 : 0.8877497607089103\n",
      "loss in epoch 244 : 0.8925851291798531\n",
      "loss in epoch 245 : 0.8917263467261132\n",
      "loss in epoch 246 : 0.8849763895602937\n",
      "loss in epoch 247 : 0.8886032827357029\n",
      "loss in epoch 248 : 0.8841797925056295\n",
      "loss in epoch 249 : 0.8926691007106862\n",
      "loss in epoch 250 : 0.8866772042944077\n",
      "loss in epoch 251 : 0.8857019759239034\n",
      "loss in epoch 252 : 0.892790749986121\n",
      "loss in epoch 253 : 0.8866690866490627\n",
      "loss in epoch 254 : 0.8901052462293747\n",
      "loss in epoch 255 : 0.90003631723688\n",
      "loss in epoch 256 : 0.8909257675739045\n",
      "loss in epoch 257 : 0.8949787388456628\n",
      "loss in epoch 258 : 0.8878772588486367\n",
      "loss in epoch 259 : 0.891897571847794\n",
      "loss in epoch 260 : 0.8837201481169843\n",
      "loss in epoch 261 : 0.8903309614100354\n",
      "loss in epoch 262 : 0.8897809906208769\n",
      "loss in epoch 263 : 0.8804272540072178\n",
      "loss in epoch 264 : 0.8930730566065362\n",
      "loss in epoch 265 : 0.8874654909397693\n",
      "loss in epoch 266 : 0.8901881004901643\n",
      "loss in epoch 267 : 0.8936234763328065\n",
      "loss in epoch 268 : 0.8983630347759166\n",
      "loss in epoch 269 : 0.888710287023098\n",
      "loss in epoch 270 : 0.8871618775611229\n",
      "loss in epoch 271 : 0.8906942301608146\n",
      "loss in epoch 272 : 0.8936055860620864\n",
      "loss in epoch 273 : 0.8863252528170322\n",
      "loss in epoch 274 : 0.8905751996851982\n",
      "loss in epoch 275 : 0.8919435118107085\n",
      "loss in epoch 276 : 0.8895665001361928\n",
      "loss in epoch 277 : 0.876602328838186\n",
      "loss in epoch 278 : 0.8840943092995501\n",
      "loss in epoch 279 : 0.8915259001102853\n",
      "loss in epoch 280 : 0.8905160515866382\n",
      "Evaluating............................................................epoch:280, time: 477.950631(s), valid (NDCG@10: 0.6077, HR@10: 0.8421)\n",
      "loss in epoch 281 : 0.8898828600315337\n",
      "loss in epoch 282 : 0.8829162311046681\n",
      "loss in epoch 283 : 0.8830064230776847\n",
      "loss in epoch 284 : 0.8945292056875026\n",
      "loss in epoch 285 : 0.8873617864669637\n",
      "loss in epoch 286 : 0.8876670865302391\n",
      "loss in epoch 287 : 0.8822436180520565\n",
      "loss in epoch 288 : 0.8885653171133487\n",
      "loss in epoch 289 : 0.8929706304631335\n",
      "loss in epoch 290 : 0.8955449499982469\n",
      "loss in epoch 291 : 0.8952062523111384\n",
      "loss in epoch 292 : 0.8900281558645532\n",
      "loss in epoch 293 : 0.8895660702218401\n",
      "loss in epoch 294 : 0.8900507333430838\n",
      "loss in epoch 295 : 0.8796375926504744\n",
      "loss in epoch 296 : 0.8891886383929151\n",
      "loss in epoch 297 : 0.8868057309313023\n",
      "loss in epoch 298 : 0.8816877185030186\n",
      "loss in epoch 299 : 0.8882169660101545\n",
      "loss in epoch 300 : 0.8813956937891372\n",
      "loss in epoch 301 : 0.8869889911184919\n",
      "loss in epoch 302 : 0.8835789421771435\n",
      "loss in epoch 303 : 0.8768194799727582\n",
      "loss in epoch 304 : 0.8945278842398461\n",
      "loss in epoch 305 : 0.8876105825951759\n",
      "loss in epoch 306 : 0.8847897826357091\n",
      "loss in epoch 307 : 0.8817113092605103\n",
      "loss in epoch 308 : 0.883502260167548\n",
      "loss in epoch 309 : 0.8827879314726972\n",
      "loss in epoch 310 : 0.8973821490368945\n",
      "loss in epoch 311 : 0.891477014156098\n",
      "loss in epoch 312 : 0.8862007247640732\n",
      "loss in epoch 313 : 0.8851900899663885\n",
      "loss in epoch 314 : 0.8876271501500556\n",
      "loss in epoch 315 : 0.8792596116979071\n",
      "loss in epoch 316 : 0.8926642143979986\n",
      "loss in epoch 317 : 0.8779340741482187\n",
      "loss in epoch 318 : 0.8911244514140677\n",
      "loss in epoch 319 : 0.8860582937585547\n",
      "loss in epoch 320 : 0.8882701067214317\n",
      "Evaluating............................................................epoch:320, time: 546.077476(s), valid (NDCG@10: 0.6150, HR@10: 0.8425)\n",
      "loss in epoch 321 : 0.8852892365861447\n",
      "loss in epoch 322 : 0.886451393999952\n",
      "loss in epoch 323 : 0.8819705083015117\n",
      "loss in epoch 324 : 0.8854526382811526\n",
      "loss in epoch 325 : 0.8889832014733172\n",
      "loss in epoch 326 : 0.8881999545908988\n",
      "loss in epoch 327 : 0.8817895574772612\n",
      "loss in epoch 328 : 0.8895724035323934\n",
      "loss in epoch 329 : 0.8826108638276445\n",
      "loss in epoch 330 : 0.8927552890270314\n",
      "loss in epoch 331 : 0.88710873177711\n",
      "loss in epoch 332 : 0.8872673739778235\n",
      "loss in epoch 333 : 0.8800279155690619\n",
      "loss in epoch 334 : 0.8891480171934087\n",
      "loss in epoch 335 : 0.8817486712273132\n",
      "loss in epoch 336 : 0.8838592425305792\n",
      "loss in epoch 337 : 0.8780262039062825\n",
      "loss in epoch 338 : 0.882613940441862\n",
      "loss in epoch 339 : 0.8755349301277323\n",
      "loss in epoch 340 : 0.888585849011198\n",
      "loss in epoch 341 : 0.8924705018388465\n",
      "loss in epoch 342 : 0.8829479445802405\n",
      "loss in epoch 343 : 0.8823435978686556\n",
      "loss in epoch 344 : 0.8754790298482205\n",
      "loss in epoch 345 : 0.8852772750753037\n",
      "loss in epoch 346 : 0.8861960236062395\n",
      "loss in epoch 347 : 0.8820429089221549\n",
      "loss in epoch 348 : 0.8856873613722781\n",
      "loss in epoch 349 : 0.883802741131884\n",
      "loss in epoch 350 : 0.8853015709430614\n",
      "loss in epoch 351 : 0.8849040523488471\n",
      "loss in epoch 352 : 0.8867162694322303\n",
      "loss in epoch 353 : 0.8828590195229713\n",
      "loss in epoch 354 : 0.8809174121694362\n",
      "loss in epoch 355 : 0.8826811617993294\n",
      "loss in epoch 356 : 0.879567341601595\n",
      "loss in epoch 357 : 0.8929029487548991\n",
      "loss in epoch 358 : 0.8914366110842279\n",
      "loss in epoch 359 : 0.889670619305144\n",
      "loss in epoch 360 : 0.8691857304978878\n",
      "Evaluating............................................................epoch:360, time: 614.014244(s), valid (NDCG@10: 0.6146, HR@10: 0.8447)\n",
      "loss in epoch 361 : 0.8885906972783677\n",
      "loss in epoch 362 : 0.8745063682819935\n",
      "loss in epoch 363 : 0.8860815687382475\n",
      "loss in epoch 364 : 0.8863295735196864\n",
      "loss in epoch 365 : 0.8788989275059802\n",
      "loss in epoch 366 : 0.8767677076319431\n",
      "loss in epoch 367 : 0.8757782451649929\n",
      "loss in epoch 368 : 0.8853332577867711\n",
      "loss in epoch 369 : 0.8795989345996937\n",
      "loss in epoch 370 : 0.8765977253305152\n",
      "loss in epoch 371 : 0.879388745794905\n",
      "loss in epoch 372 : 0.8828722644359508\n",
      "loss in epoch 373 : 0.8893052504417744\n",
      "loss in epoch 374 : 0.8873950078132304\n",
      "loss in epoch 375 : 0.878456785323772\n",
      "loss in epoch 376 : 0.8766245562979516\n",
      "loss in epoch 377 : 0.8831036065487151\n",
      "loss in epoch 378 : 0.8769736683115046\n",
      "loss in epoch 379 : 0.8806859863565323\n",
      "loss in epoch 380 : 0.8834838359913928\n",
      "loss in epoch 381 : 0.8757384000940526\n",
      "loss in epoch 382 : 0.8869579447076675\n",
      "loss in epoch 383 : 0.8890288332675366\n",
      "loss in epoch 384 : 0.8693794658843507\n",
      "loss in epoch 385 : 0.8892785044426613\n",
      "loss in epoch 386 : 0.8819231390953064\n",
      "loss in epoch 387 : 0.8803075006667603\n",
      "loss in epoch 388 : 0.8798307345268574\n",
      "loss in epoch 389 : 0.8828825671622094\n",
      "loss in epoch 390 : 0.8832992251883162\n",
      "loss in epoch 391 : 0.8734586073997173\n",
      "loss in epoch 392 : 0.8821550480862881\n",
      "loss in epoch 393 : 0.8729602060419448\n",
      "loss in epoch 394 : 0.8890122796626801\n",
      "loss in epoch 395 : 0.8797202059563171\n",
      "loss in epoch 396 : 0.8875320071869708\n",
      "loss in epoch 397 : 0.8790200314623244\n",
      "loss in epoch 398 : 0.8786518091851092\n",
      "loss in epoch 399 : 0.8779623850863031\n",
      "loss in epoch 400 : 0.8820218887734921\n",
      "Evaluating............................................................epoch:400, time: 681.999946(s), valid (NDCG@10: 0.6169, HR@10: 0.8425)\n",
      "loss in epoch 401 : 0.8724933139821316\n",
      "loss in epoch 402 : 0.8802353486101678\n",
      "loss in epoch 403 : 0.8820788276956436\n",
      "loss in epoch 404 : 0.8842725867920733\n",
      "loss in epoch 405 : 0.8750323462993541\n",
      "loss in epoch 406 : 0.8745342974967145\n",
      "loss in epoch 407 : 0.8808861844083096\n",
      "loss in epoch 408 : 0.8788294360992757\n",
      "loss in epoch 409 : 0.8746059866661721\n",
      "loss in epoch 410 : 0.8750233789707752\n",
      "loss in epoch 411 : 0.8788221313598308\n",
      "loss in epoch 412 : 0.8820313291346773\n",
      "loss in epoch 413 : 0.8778616988912542\n",
      "loss in epoch 414 : 0.881983053176961\n",
      "loss in epoch 415 : 0.878394809175045\n",
      "loss in epoch 416 : 0.880198876908485\n",
      "loss in epoch 417 : 0.8770674200768166\n",
      "loss in epoch 418 : 0.8866125269139067\n",
      "loss in epoch 419 : 0.8828190793382361\n",
      "loss in epoch 420 : 0.874643826738317\n",
      "loss in epoch 421 : 0.8752806934904545\n",
      "loss in epoch 422 : 0.8843603806292757\n",
      "loss in epoch 423 : 0.8748141527175903\n",
      "loss in epoch 424 : 0.8776648158722735\n",
      "loss in epoch 425 : 0.8847941756248474\n",
      "loss in epoch 426 : 0.8784638884219718\n",
      "loss in epoch 427 : 0.8875424164406797\n",
      "loss in epoch 428 : 0.8829998640303917\n",
      "loss in epoch 429 : 0.8750922806719517\n",
      "loss in epoch 430 : 0.8924859102736128\n",
      "loss in epoch 431 : 0.8749062926211255\n",
      "loss in epoch 432 : 0.8736918987111842\n",
      "loss in epoch 433 : 0.8811215098868025\n",
      "loss in epoch 434 : 0.8747432637721935\n",
      "loss in epoch 435 : 0.8797246382591573\n",
      "loss in epoch 436 : 0.8777525780048776\n",
      "loss in epoch 437 : 0.8730752442745452\n",
      "loss in epoch 438 : 0.8796498458436195\n",
      "loss in epoch 439 : 0.8758507903586042\n",
      "loss in epoch 440 : 0.87808210925853\n",
      "Evaluating............................................................epoch:440, time: 749.947796(s), valid (NDCG@10: 0.6171, HR@10: 0.8425)\n",
      "loss in epoch 441 : 0.8788541885132485\n",
      "loss in epoch 442 : 0.8862348186208847\n",
      "loss in epoch 443 : 0.879985399702762\n",
      "loss in epoch 444 : 0.8775575160980225\n",
      "loss in epoch 445 : 0.8811515303368264\n",
      "loss in epoch 446 : 0.8749855680668608\n",
      "loss in epoch 447 : 0.8877240698388282\n",
      "loss in epoch 448 : 0.883930688208722\n",
      "loss in epoch 449 : 0.881653055231622\n",
      "loss in epoch 450 : 0.8817346514539516\n",
      "loss in epoch 451 : 0.8805697953447382\n",
      "loss in epoch 452 : 0.8782245643595432\n",
      "loss in epoch 453 : 0.8795452561784298\n",
      "loss in epoch 454 : 0.8787079275922572\n",
      "loss in epoch 455 : 0.8780856094461806\n",
      "loss in epoch 456 : 0.8821323437893644\n",
      "loss in epoch 457 : 0.8716111690440076\n",
      "loss in epoch 458 : 0.8798335854043352\n",
      "loss in epoch 459 : 0.8797038831609361\n",
      "loss in epoch 460 : 0.868744803235886\n",
      "loss in epoch 461 : 0.886692149842039\n",
      "loss in epoch 462 : 0.8814882978479913\n",
      "loss in epoch 463 : 0.8775963085762998\n",
      "loss in epoch 464 : 0.8853980315492508\n",
      "loss in epoch 465 : 0.8788592485671348\n",
      "loss in epoch 466 : 0.8743546795337758\n",
      "loss in epoch 467 : 0.8816894698650279\n",
      "loss in epoch 468 : 0.8722462755568484\n",
      "loss in epoch 469 : 0.8755216902874886\n",
      "loss in epoch 470 : 0.883332742021439\n",
      "loss in epoch 471 : 0.8831687582300064\n",
      "loss in epoch 472 : 0.8735550781513782\n",
      "loss in epoch 473 : 0.8881020305004526\n",
      "loss in epoch 474 : 0.8765238903938456\n",
      "loss in epoch 475 : 0.8778401562508117\n",
      "loss in epoch 476 : 0.8802993272213225\n",
      "loss in epoch 477 : 0.8865169591092049\n",
      "loss in epoch 478 : 0.870184432952962\n",
      "loss in epoch 479 : 0.8832686873192482\n",
      "loss in epoch 480 : 0.8797135429179415\n",
      "Evaluating............................................................epoch:480, time: 817.729824(s), valid (NDCG@10: 0.6219, HR@10: 0.8440)\n",
      "loss in epoch 481 : 0.8727336284962106\n",
      "loss in epoch 482 : 0.8791168636464058\n",
      "loss in epoch 483 : 0.8807173244496609\n",
      "loss in epoch 484 : 0.8779861280258666\n",
      "loss in epoch 485 : 0.8757311422774132\n",
      "loss in epoch 486 : 0.8750871204315348\n",
      "loss in epoch 487 : 0.868020920043296\n",
      "loss in epoch 488 : 0.8763822456623646\n",
      "loss in epoch 489 : 0.876622219034966\n",
      "loss in epoch 490 : 0.878392238566216\n",
      "loss in epoch 491 : 0.8809291606253766\n",
      "loss in epoch 492 : 0.8731570713063503\n",
      "loss in epoch 493 : 0.8797428151394459\n",
      "loss in epoch 494 : 0.8772944085141445\n",
      "loss in epoch 495 : 0.873354693676563\n",
      "loss in epoch 496 : 0.8796172256165362\n",
      "loss in epoch 497 : 0.8751551318675914\n",
      "loss in epoch 498 : 0.873890238873502\n",
      "loss in epoch 499 : 0.8794114323372536\n",
      "loss in epoch 500 : 0.8797803318246882\n",
      "loss in epoch 501 : 0.8792239376839172\n",
      "loss in epoch 502 : 0.8779096375120446\n",
      "loss in epoch 503 : 0.8859582074145054\n",
      "loss in epoch 504 : 0.8733148625556458\n",
      "loss in epoch 505 : 0.8829703115402384\n",
      "loss in epoch 506 : 0.8778076666466733\n",
      "loss in epoch 507 : 0.8793529145261074\n",
      "loss in epoch 508 : 0.876907745574383\n",
      "loss in epoch 509 : 0.8800884840336252\n",
      "loss in epoch 510 : 0.8786401254065493\n",
      "loss in epoch 511 : 0.8856847717406902\n",
      "loss in epoch 512 : 0.8733191616991733\n",
      "loss in epoch 513 : 0.8723686603789634\n",
      "loss in epoch 514 : 0.8775697515365926\n",
      "loss in epoch 515 : 0.8804925971842826\n",
      "loss in epoch 516 : 0.8862612044557612\n",
      "loss in epoch 517 : 0.8731683581433398\n",
      "loss in epoch 518 : 0.8746609751214373\n",
      "loss in epoch 519 : 0.8799200413074899\n",
      "loss in epoch 520 : 0.8819934839897967\n",
      "Evaluating............................................................epoch:520, time: 885.592414(s), valid (NDCG@10: 0.6225, HR@10: 0.8442)\n",
      "loss in epoch 521 : 0.8781172201988545\n",
      "loss in epoch 522 : 0.8806707351765735\n",
      "loss in epoch 523 : 0.8694823523785206\n",
      "loss in epoch 524 : 0.8758597551508153\n",
      "loss in epoch 525 : 0.8820840094951873\n",
      "loss in epoch 526 : 0.8746752079496992\n",
      "loss in epoch 527 : 0.8767209268630819\n",
      "loss in epoch 528 : 0.8754164386302867\n",
      "loss in epoch 529 : 0.8748808756787726\n",
      "loss in epoch 530 : 0.8794831975977472\n",
      "loss in epoch 531 : 0.8838500951198821\n",
      "loss in epoch 532 : 0.875711036489365\n",
      "loss in epoch 533 : 0.8804616611054603\n",
      "loss in epoch 534 : 0.8737896744241106\n",
      "loss in epoch 535 : 0.8673433306369376\n",
      "loss in epoch 536 : 0.8792598463119344\n",
      "loss in epoch 537 : 0.8719781611828094\n",
      "loss in epoch 538 : 0.8747453423256569\n",
      "loss in epoch 539 : 0.8806421921608296\n",
      "loss in epoch 540 : 0.8752626687922376\n",
      "loss in epoch 541 : 0.8800207997890229\n",
      "loss in epoch 542 : 0.8814425595263218\n",
      "loss in epoch 543 : 0.8736237477748952\n",
      "loss in epoch 544 : 0.87730743276312\n",
      "loss in epoch 545 : 0.8771401973480873\n",
      "loss in epoch 546 : 0.878799040266808\n",
      "loss in epoch 547 : 0.8889538151152591\n",
      "loss in epoch 548 : 0.8796721316398458\n",
      "loss in epoch 549 : 0.8742616480969368\n",
      "loss in epoch 550 : 0.873136318744497\n",
      "loss in epoch 551 : 0.8765024859854516\n",
      "loss in epoch 552 : 0.8758934888433902\n",
      "loss in epoch 553 : 0.8689611528782134\n",
      "loss in epoch 554 : 0.881738359623767\n",
      "loss in epoch 555 : 0.8713745183133065\n",
      "loss in epoch 556 : 0.8810987535943376\n",
      "loss in epoch 557 : 0.8840555434531354\n",
      "loss in epoch 558 : 0.8727433136168946\n",
      "loss in epoch 559 : 0.8769656635345296\n",
      "loss in epoch 560 : 0.8812042195746239\n",
      "Evaluating............................................................epoch:560, time: 953.585073(s), valid (NDCG@10: 0.6212, HR@10: 0.8465)\n",
      "loss in epoch 561 : 0.8687672437505519\n",
      "loss in epoch 562 : 0.8716559752504877\n",
      "loss in epoch 563 : 0.8817967468119682\n",
      "loss in epoch 564 : 0.8745635557681957\n",
      "loss in epoch 565 : 0.8720742286519801\n",
      "loss in epoch 566 : 0.8805453853404268\n",
      "loss in epoch 567 : 0.8743052000695086\n",
      "loss in epoch 568 : 0.8828343647591611\n",
      "loss in epoch 569 : 0.8831902780431382\n",
      "loss in epoch 570 : 0.8821086820135725\n",
      "loss in epoch 571 : 0.8661853658392075\n",
      "loss in epoch 572 : 0.8770771457793864\n",
      "loss in epoch 573 : 0.8701130265885211\n",
      "loss in epoch 574 : 0.8769458950834071\n",
      "loss in epoch 575 : 0.8736320734024048\n",
      "loss in epoch 576 : 0.8825048081418301\n",
      "loss in epoch 577 : 0.8729024478729736\n",
      "loss in epoch 578 : 0.8759659579459657\n",
      "loss in epoch 579 : 0.8762321560940844\n",
      "loss in epoch 580 : 0.8816625957793378\n",
      "loss in epoch 581 : 0.8804445976906634\n",
      "loss in epoch 582 : 0.8765218815904983\n",
      "loss in epoch 583 : 0.8831313201721679\n",
      "loss in epoch 584 : 0.876354419170542\n",
      "loss in epoch 585 : 0.873276117000174\n",
      "loss in epoch 586 : 0.8712257996518561\n",
      "loss in epoch 587 : 0.879679005196754\n",
      "loss in epoch 588 : 0.8830246265898359\n",
      "loss in epoch 589 : 0.8732797363971142\n",
      "loss in epoch 590 : 0.8751513755067866\n",
      "loss in epoch 591 : 0.8747351296404575\n",
      "loss in epoch 592 : 0.8708915152448289\n",
      "loss in epoch 593 : 0.8747653212953121\n",
      "loss in epoch 594 : 0.8761502719940023\n",
      "loss in epoch 595 : 0.8766539528014812\n",
      "loss in epoch 596 : 0.8808682928694055\n",
      "loss in epoch 597 : 0.8689288167243309\n",
      "loss in epoch 598 : 0.8752807125132135\n",
      "loss in epoch 599 : 0.8794709469409699\n",
      "loss in epoch 600 : 0.8694528721748515\n",
      "Evaluating............................................................epoch:600, time: 1021.409624(s), valid (NDCG@10: 0.6248, HR@10: 0.8465)\n"
     ]
    }
   ],
   "source": [
    "import time\r\n",
    "T = 0.0\r\n",
    "t0 = time.time()\r\n",
    "\r\n",
    "for epoch in range(num_epochs+1 ):\r\n",
    "    loss_e = 0\r\n",
    "    # for i, u, seq, pos, neg in  tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\r\n",
    "    for i in range(num_batch):\r\n",
    "        u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\r\n",
    "        \r\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\r\n",
    "        pos_logits, neg_logits = model(u, seq, pos, neg)\r\n",
    "        \r\n",
    "        pos_labels, neg_labels = paddle.ones(pos_logits.shape), paddle.zeros(neg_logits.shape)\r\n",
    "        # print(\"\\neye ball check raw_logits:\"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0\r\n",
    "        adam_optimizer.clear_grad()\r\n",
    "        indices = paddle.to_tensor(np.where(pos != 0,1,0)).astype(paddle.float32)\r\n",
    "\r\n",
    "        pos_logits = paddle.multiply(pos_logits, indices)\r\n",
    "        pos_labels = paddle.multiply(pos_labels, indices)\r\n",
    "        neg_logits = paddle.multiply(neg_logits, indices)\r\n",
    "        neg_labels = paddle.multiply(neg_labels, indices)\r\n",
    "\r\n",
    "        loss = bce_criterion(pos_logits, pos_labels)\r\n",
    "        loss += bce_criterion(neg_logits, neg_labels)\r\n",
    "\r\n",
    "        for param in model.item_emb.parameters(): loss += l2_emb * paddle.norm(param)\r\n",
    "        loss.backward()\r\n",
    "        adam_optimizer.step()\r\n",
    "        loss_e += loss.item()\r\n",
    "    print(\"loss in epoch {} : {}\".format(epoch, loss_e/num_batch)) # expected 0.4~0.6 after init few epochs\r\n",
    "\r\n",
    "    if epoch % 40 == 0 and epoch!=0:\r\n",
    "        model.eval()\r\n",
    "        t1 = time.time() - t0\r\n",
    "        T += t1\r\n",
    "        print('Evaluating', end='')\r\n",
    "        #t_test = evaluate(model, user_train, user_valid, user_test, usernum, itemnum, maxlen)\r\n",
    "        t_valid = evaluate_valid(model, user_train, user_valid, user_test, usernum, itemnum, maxlen)\r\n",
    "        print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f)'\r\n",
    "                % (epoch, T, t_valid[0], t_valid[1]))\r\n",
    "\r\n",
    "        f.write(str(t_valid) + '\\n')\r\n",
    "        f.flush()\r\n",
    "        t0 = time.time()\r\n",
    "        model.train()\r\n",
    "\r\n",
    "    if epoch == num_epochs:\r\n",
    "        folder = dataset\r\n",
    "        fname = 'SASRec.epoch={}.lr={}.layer={}.head={}.hidden={}.maxlen={}.pdparams'\r\n",
    "        fname = fname.format(num_epochs, lr, num_blocks, num_heads, hidden_units, maxlen)\r\n",
    "        paddle.save(model.state_dict(), os.path.join(folder, fname))\r\n",
    "\r\n",
    "f.close()\r\n",
    "sampler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................Valid: NDCG@10: 0.5912, HR@10: 0.8315 /n Test: NDCG@10: 0.5713, HR@10: 0.8103 \n"
     ]
    }
   ],
   "source": [
    "# load model and eval\r\n",
    "\r\n",
    "state_dict = paddle.load('ml-1m/SASRec.epoch=600.lr=0.001.layer=2.head=1.hidden=50.maxlen=200.pdparams')\r\n",
    "model_eval = SASRec(usernum, itemnum, batch_size, lr, maxlen, hidden_units, num_blocks, num_epochs, num_heads, dropout_rate, l2_emb)\r\n",
    "model_eval.set_state_dict(state_dict)\r\n",
    "\r\n",
    "valid = evaluate_valid(model_eval, user_train, user_valid, user_test, usernum, itemnum, maxlen)\r\n",
    "test = evaluate(model_eval, user_train, user_valid, user_test, usernum, itemnum, maxlen)\r\n",
    "print('Valid: NDCG@10: %.4f, HR@10: %.4f /n Test: NDCG@10: %.4f, HR@10: %.4f ' %(valid[0], valid[1], test[0], test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
